{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Anomaly Detection with PyTorch\n",
    "\n",
    "This notebook implements an Elasticsearch-inspired hybrid anomaly detection pipeline using PyTorch instead of TensorFlow/Keras. The implementation supports multidimensional sensor inputs and combines several anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pyod.models.ecod import ECOD  # Unsupervised anomaly detection\n",
    "from pyod.models.combination import aom, moa, average  # Ensemble methods\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === CONFIG ===\n",
    "csv_path = \"../test_csv/motor_power_high.csv\"  # Update this path to your data\n",
    "sensors_to_test = [\"temperature\", \"vibration\", \"rpm\", \"power\"]\n",
    "window_size = 30\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "contamination = 0.05  # Expected anomaly rate\n",
    "feature_columns = [\"feature_0\", \"feature_1\", \"feature_2\"]  # Handle multi-dimensional features\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create folders to save artifacts\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "os.makedirs(\"anomaly_reports\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch LSTM Autoencoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states shape: (batch_size, seq_len, hidden_size)\n",
    "        energy = self.attention(hidden_states)\n",
    "        # energy shape: (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(energy, dim=1)\n",
    "        # Apply attention weights to hidden states\n",
    "        context = hidden_states * attention_weights\n",
    "        return context\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = AttentionLayer(hidden_dim1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden_dim1,\n",
    "            hidden_size=hidden_dim2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Output layer (recreates the original input)\n",
    "        self.output_layer = nn.Linear(hidden_dim2, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        # lstm1_out shape: (batch_size, seq_len, hidden_dim1)\n",
    "        \n",
    "        # Attention\n",
    "        attention_out = self.attention(lstm1_out)\n",
    "        # attention_out shape: (batch_size, seq_len, hidden_dim1)\n",
    "        \n",
    "        # Decoder\n",
    "        lstm2_out, _ = self.lstm2(attention_out)\n",
    "        # lstm2_out shape: (batch_size, seq_len, hidden_dim2)\n",
    "        \n",
    "        # Output projection\n",
    "        reconstructed = self.output_layer(lstm2_out)\n",
    "        # reconstructed shape: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        return reconstructed\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Compatibility method to match Keras API\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(x, torch.Tensor):\n",
    "                x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            return self(x).cpu().numpy()\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model parameters\"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.2):\n",
    "        \"\"\"Load the model parameters\"\"\"\n",
    "        model = cls(input_dim, hidden_dim1, hidden_dim2, dropout_rate)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def create_weighted_sampler(pseudo_labels, weight_factor=3.0):\n",
    "    \"\"\"Create a weighted sampler to focus on potential anomalies\"\"\"\n",
    "    sample_weights = np.ones(len(pseudo_labels))\n",
    "    sample_weights[pseudo_labels == 1] = weight_factor\n",
    "    return WeightedRandomSampler(sample_weights, len(sample_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, sample_weights=None, epochs=100, batch_size=64, validation_split=0.1, patience=5):\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Split into training and validation\n",
    "    val_size = int(len(X_train) * validation_split)\n",
    "    train_size = len(X_train) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        TensorDataset(X_train_tensor, X_train_tensor), \n",
    "        [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create samplers\n",
    "    if sample_weights is not None:\n",
    "        # Handle weighted sampling for anomalies\n",
    "        train_indices = train_dataset.indices\n",
    "        train_weights = sample_weights[train_indices]\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            train_weights, \n",
    "            num_samples=len(train_indices), \n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Early stopping setup\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training history (like Keras)\n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Average losses\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Update history\n",
    "        history['loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Implementation: Elastic Hybrid Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_hybrid_detection(sensor_name, sensor_df):\n",
    "    print(f\"\\nðŸ” Processing sensor: {sensor_name}\")\n",
    "    \n",
    "    # Identify available features - now supports multiple dimensions\n",
    "    actual_feature_columns = [col for col in feature_columns if col in sensor_df.columns]\n",
    "    print(f\"Using features: {actual_feature_columns}\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    le = LabelEncoder()\n",
    "    sensor_df[\"state_encoded\"] = le.fit_transform(sensor_df[\"state\"])\n",
    "    scaler = RobustScaler()  # More robust to outliers\n",
    "    \n",
    "    # Select available features plus state_encoded\n",
    "    feature_cols = actual_feature_columns + [\"state_encoded\"]\n",
    "    features = scaler.fit_transform(sensor_df[feature_cols])\n",
    "    timestamps = sensor_df[\"timestamp\"].values\n",
    "    \n",
    "    # Create windows\n",
    "    X, time_windows = [], []\n",
    "    for i in range(len(features) - window_size):\n",
    "        X.append(features[i:i+window_size])\n",
    "        time_windows.append(timestamps[i:i+window_size])\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Split into train/test (no labels needed)\n",
    "    X_train, X_test, time_train, time_test = train_test_split(\n",
    "        X, time_windows, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # === STAGE 1: Unsupervised Ensemble ===\n",
    "    print(\"\\n=== STAGE 1: Unsupervised Ensemble Detection ===\")\n",
    "    \n",
    "    # 1A: Statistical Decomposition (STL) - only works on 1D time series\n",
    "    # Use the first feature for decomposition\n",
    "    stl = STL(sensor_df[actual_feature_columns[0]], period=24)\n",
    "    stl_results = stl.fit()\n",
    "    residual_scores = np.abs(stl_results.resid)\n",
    "    residual_scores = (residual_scores - residual_scores.min()) / (residual_scores.max() - residual_scores.min())\n",
    "    \n",
    "    # 1B: Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    flat_features = X_train.reshape(X_train.shape[0], -1)\n",
    "    iso_scores = -iso_forest.fit_predict(flat_features)\n",
    "    \n",
    "    # 1C: One-Class SVM\n",
    "    oc_svm = OneClassSVM(nu=contamination, kernel=\"rbf\")\n",
    "    svm_scores = -oc_svm.fit_predict(flat_features)\n",
    "    \n",
    "    # 1D: DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(flat_features)\n",
    "    dbscan_scores = (dbscan_labels == -1).astype(int)\n",
    "    \n",
    "    # 1E: ECOD (Elastic-inspired)\n",
    "    ecod = ECOD(contamination=contamination)\n",
    "    ecod_scores = ecod.fit_predict(flat_features)\n",
    "    \n",
    "    # Combine scores using Elasticsearch-like aggregation\n",
    "    unsupervised_scores = (\n",
    "        0.25 * residual_scores[-len(X_train):] + \n",
    "        0.25 * iso_scores + \n",
    "        0.20 * svm_scores + \n",
    "        0.15 * dbscan_scores +\n",
    "        0.15 * ecod_scores\n",
    "    )\n",
    "    \n",
    "    # === STAGE 2: Semi-Supervised Learning ===\n",
    "    print(\"\\n=== STAGE 2: Semi-Supervised Learning ===\")\n",
    "    \n",
    "    # Create pseudo-labels from unsupervised results\n",
    "    threshold_unsupervised = np.percentile(unsupervised_scores, 100*(1-contamination))\n",
    "    pseudo_labels = (unsupervised_scores > threshold_unsupervised).astype(int)\n",
    "    \n",
    "    # Initialize PyTorch model\n",
    "    input_dim = X.shape[2]  # Number of features\n",
    "    model = LSTMAutoencoder(input_dim=input_dim).to(device)\n",
    "    \n",
    "    # Train using our custom training function with sample weights\n",
    "    model, history = train_model(\n",
    "        model, \n",
    "        X_train, \n",
    "        sample_weights=np.ones(len(X_train)) * (pseudo_labels * 2 + 1),  # Higher weight for anomalies\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # === STAGE 3: Hybrid Scoring ===\n",
    "    print(\"\\n=== STAGE 3: Hybrid Scoring ===\")\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_reconstructions = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    test_mse = np.mean(np.power(X_test - test_reconstructions, 2), axis=(1, 2))\n",
    "    \n",
    "    # Generate unsupervised scores for test set\n",
    "    flat_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    test_iso = -iso_forest.predict(flat_test)\n",
    "    test_svm = -oc_svm.predict(flat_test)\n",
    "    test_dbscan = (dbscan.fit_predict(flat_test) == -1).astype(int)\n",
    "    test_ecod = ecod.predict(flat_test)\n",
    "    test_residual = residual_scores[-len(X_test):]\n",
    "    \n",
    "    # Combine scores using dynamic weighting\n",
    "    test_unsupervised = (\n",
    "        0.25 * test_residual + \n",
    "        0.25 * test_iso + \n",
    "        0.20 * test_svm + \n",
    "        0.15 * test_dbscan +\n",
    "        0.15 * test_ecod\n",
    "    )\n",
    "    \n",
    "    # Elasticsearch-style hybrid scoring\n",
    "    hybrid_scores = 0.6 * test_mse + 0.4 * test_unsupervised\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    threshold = np.percentile(hybrid_scores, 100*(1-contamination))\n",
    "    \n",
    "    # If we have some labels for evaluation (optional)\n",
    "    if \"is_anomaly\" in sensor_df.columns:\n",
    "        y_true = []\n",
    "        for i in range(len(X) - len(X_test), len(X)):\n",
    "            y_true.append(int(sensor_df[\"is_anomaly\"].iloc[i:i+window_size].any()))\n",
    "        y_true = np.array(y_true[-len(X_test):])  # Align with test set\n",
    "        \n",
    "        if np.sum(y_true) > 0:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true, hybrid_scores)\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            threshold = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "    # Final predictions\n",
    "    y_pred = (hybrid_scores > threshold).astype(int)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = f\"saved_models/{sensor_name}_elastic_hybrid.pt\"\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ… Hybrid model saved to {model_path}\")\n",
    "    \n",
    "    # === VISUALIZATION & REPORTING ===\n",
    "    test_timestamps = [window[-1] for window in time_test]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(test_timestamps, hybrid_scores, 'b-', label='Hybrid Anomaly Score')\n",
    "    \n",
    "    if \"is_anomaly\" in sensor_df.columns and np.sum(y_true) > 0:\n",
    "        plt.scatter(test_timestamps[y_true == 1], hybrid_scores[y_true == 1],\n",
    "                   color='red', label='True Anomalies')\n",
    "    \n",
    "    plt.scatter(test_timestamps[y_pred == 1], hybrid_scores[y_pred == 1],\n",
    "               color='black', marker='x', s=100, label='Predicted Anomalies')\n",
    "    plt.axhline(threshold, color='green', linestyle='--', \n",
    "               label=f'Threshold: {threshold:.4f}')\n",
    "    \n",
    "    plt.title(f\"Elastic-Inspired Hybrid Detection - {sensor_name}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"Anomaly Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = f\"anomaly_reports/{sensor_name}_hybrid_results.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"âœ… Results plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the training history\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training History - {sensor_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"anomaly_reports/{sensor_name}_training_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'threshold': threshold,\n",
    "        'unsupervised_models': (iso_forest, oc_svm, dbscan, ecod)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN EXECUTION ===\n",
    "for sensor_name in sensors_to_test:\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"timestamp\"])\n",
    "    sensor_df = df[df[\"sensor\"] == sensor_name].copy()\n",
    "    sensor_df.sort_values(by=\"timestamp\", inplace=True)\n",
    "    \n",
    "    # Run the hybrid pipeline\n",
    "    results = elastic_hybrid_detection(sensor_name, sensor_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(model_path, input_data, scaler, threshold, unsupervised_models=None):\n",
    "    \"\"\"Process new data for anomaly detection using a trained model\"\"\"\n",
    "    # Scale the input data\n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    \n",
    "    # Create windows\n",
    "    X = []\n",
    "    for i in range(len(scaled_data) - window_size):\n",
    "        X.append(scaled_data[i:i+window_size])\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Load the model\n",
    "    input_dim = X.shape[2]  # Number of features\n",
    "    model = LSTMAutoencoder.load(model_path, input_dim=input_dim).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    